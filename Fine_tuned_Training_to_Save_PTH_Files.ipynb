{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of gpu: 1\n",
      "gpu name: NVIDIA RTX A6000\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"number of gpu:\", torch.cuda.device_count())\n",
    "print(\"gpu name:\", torch.cuda.get_device_name())\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 VGG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vb11574\\AppData\\Local\\miniconda3\\envs\\salmo\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\vb11574\\AppData\\Local\\miniconda3\\envs\\salmo\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Validation Loss: 0.6385\n",
      "Epoch 2/30, Validation Loss: 0.6416\n",
      "Epoch 3/30, Validation Loss: 0.6236\n",
      "Epoch 4/30, Validation Loss: 0.6578\n",
      "Epoch 5/30, Validation Loss: 0.6622\n",
      "Epoch 6/30, Validation Loss: 0.6913\n",
      "Epoch 7/30, Validation Loss: 0.7156\n",
      "Epoch 8/30, Validation Loss: 0.7141\n",
      "Early stopping triggered.\n",
      "Epoch 1/70, Validation Loss: 0.6746\n",
      "Epoch 2/70, Validation Loss: 0.7653\n",
      "Epoch 3/70, Validation Loss: 0.7863\n",
      "Epoch 4/70, Validation Loss: 0.9367\n",
      "Epoch 5/70, Validation Loss: 0.9944\n",
      "Epoch 6/70, Validation Loss: 1.0492\n",
      "Early stopping triggered.\n",
      "✅ Final fine-tuned model saved at: C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/Models/American_to_African/Vgg16_ft_final_model.pth\n",
      "✅ Test Accuracy after Fine-Tuning: 0.3886\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "FROZEN_EPOCHS = 30\n",
    "FINE_TUNE_EPOCHS = 70\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "FINE_TUNE_LR = 1e-5\n",
    "BEST_MODEL_PATH = 'C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/Models/American_to_African/new/Vgg16_ft_best_model.pth'\n",
    "FINAL_MODEL_PATH = 'C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/Models/American_to_African/new/Vgg16_ft_final_model.pth'\n",
    "\n",
    "# Transforms\n",
    "transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset & Dataloaders (Updated paths)\n",
    "train_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/DatasetAfricantoAmerican/train', transform=transform)\n",
    "val_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/DatasetAmericantoAfrican/valid', transform=transform)\n",
    "test_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/DatasetAmericantoAfrican/test', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Load pretrained VGG19 and freeze feature layers\n",
    "base_model = models.vgg16(pretrained=True)\n",
    "for param in base_model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace only the classifier (keep avgpool as is)\n",
    "base_model.classifier = nn.Sequential(\n",
    "    nn.Linear(512 * 7 * 7, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(512, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "model = base_model.to(device)\n",
    "\n",
    "# Loss & optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "\n",
    "# Early Stopping Class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, path=BEST_MODEL_PATH):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.best_model = None\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            torch.save(self.best_model, self.path)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        return self.counter >= self.patience\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "# Accuracy Function\n",
    "def compute_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = (outputs > 0.5).int().squeeze()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs):\n",
    "    early_stopping = EarlyStopping(patience=5)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        val_loss = evaluate(model, val_loader, criterion)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if early_stopping(val_loss, model):\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Train model with frozen layers\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, FROZEN_EPOCHS)\n",
    "\n",
    "# Load best model before fine-tuning\n",
    "model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "\n",
    "# Unfreeze last 50 conv layers\n",
    "for param in list(model.features.parameters())[-50:]:\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Recompile optimizer with updated params\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=FINE_TUNE_LR)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "\n",
    "# Fine-tune the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, FINE_TUNE_EPOCHS)\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), FINAL_MODEL_PATH)\n",
    "print(f\"✅ Final fine-tuned model saved at: {FINAL_MODEL_PATH}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_acc = compute_accuracy(model, test_loader)\n",
    "print(f\"✅ Test Accuracy after Fine-Tuning: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vb11574\\AppData\\Local\\miniconda3\\envs\\salmo\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Validation Loss: 0.6497\n",
      "Epoch 2/30, Validation Loss: 0.6509\n",
      "Epoch 3/30, Validation Loss: 0.6538\n",
      "Epoch 4/30, Validation Loss: 0.6966\n",
      "Epoch 5/30, Validation Loss: 0.6902\n",
      "Epoch 6/30, Validation Loss: 0.6701\n",
      "Early stopping triggered.\n",
      "Epoch 1/70, Validation Loss: 0.6338\n",
      "Epoch 2/70, Validation Loss: 0.6609\n",
      "Epoch 3/70, Validation Loss: 0.7250\n",
      "Epoch 4/70, Validation Loss: 0.8342\n",
      "Epoch 5/70, Validation Loss: 1.1709\n",
      "Epoch 6/70, Validation Loss: 1.0216\n",
      "Early stopping triggered.\n",
      "✅ Final fine-tuned model saved at: C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/Models/American_to_African/Vgg19_ft_final_model.pth\n",
      "✅ Test Accuracy after Fine-Tuning: 0.3185\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "FROZEN_EPOCHS = 30\n",
    "FINE_TUNE_EPOCHS = 70\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "FINE_TUNE_LR = 1e-5\n",
    "BEST_MODEL_PATH = 'C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/Models/American_to_African/Vgg19_ft_best_model.pth'\n",
    "FINAL_MODEL_PATH = 'C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/Models/American_to_African/Vgg19_ft_final_model.pth'\n",
    "\n",
    "# Transforms\n",
    "transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor(),\n",
    "    Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset & Dataloaders\n",
    "train_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/DatasetAmericantoAfrican/train', transform=transform)\n",
    "val_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/DatasetAmericantoAfrican/valid', transform=transform)\n",
    "test_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/DatasetAmericantoAfrican/test', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Load pretrained VGG19 and freeze feature layers\n",
    "base_model = models.vgg19(pretrained=True)\n",
    "for param in base_model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace only the classifier (keep avgpool as is)\n",
    "base_model.classifier = nn.Sequential(\n",
    "    nn.Linear(512 * 7 * 7, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(512, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "model = base_model.to(device)\n",
    "\n",
    "# Loss & optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "\n",
    "# Early Stopping Class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, path=BEST_MODEL_PATH):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.best_model = None\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            torch.save(self.best_model, self.path)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        return self.counter >= self.patience\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "# Accuracy Function\n",
    "def compute_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = (outputs > 0.5).int().squeeze()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs):\n",
    "    early_stopping = EarlyStopping(patience=5)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        val_loss = evaluate(model, val_loader, criterion)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if early_stopping(val_loss, model):\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Train model with frozen layers\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, FROZEN_EPOCHS)\n",
    "\n",
    "# Load best model before fine-tuning\n",
    "model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "\n",
    "# Unfreeze last 50 conv layers\n",
    "for param in list(model.features.parameters())[-50:]:\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Recompile optimizer with updated params\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=FINE_TUNE_LR)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "\n",
    "# Fine-tune the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, FINE_TUNE_EPOCHS)\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), FINAL_MODEL_PATH)\n",
    "print(f\"✅ Final fine-tuned model saved at: {FINAL_MODEL_PATH}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_acc = compute_accuracy(model, test_loader)\n",
    "print(f\"✅ Test Accuracy after Fine-Tuning: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 MobileNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 MobileNet v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vb11574\\AppData\\Local\\miniconda3\\envs\\salmo\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to C:\\Users\\vb11574/.cache\\torch\\hub\\checkpoints\\mobilenet_v2-b0353104.pth\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Validation Loss: 0.6553\n",
      "Epoch 2/30, Validation Loss: 0.6463\n",
      "Epoch 3/30, Validation Loss: 0.6428\n",
      "Epoch 4/30, Validation Loss: 0.6384\n",
      "Epoch 5/30, Validation Loss: 0.6390\n",
      "Epoch 6/30, Validation Loss: 0.6411\n",
      "Epoch 7/30, Validation Loss: 0.6314\n",
      "Epoch 8/30, Validation Loss: 0.6292\n",
      "Epoch 9/30, Validation Loss: 0.6554\n",
      "Epoch 10/30, Validation Loss: 0.6309\n",
      "Epoch 11/30, Validation Loss: 0.6245\n",
      "Epoch 12/30, Validation Loss: 0.6252\n",
      "Epoch 13/30, Validation Loss: 0.6330\n",
      "Epoch 14/30, Validation Loss: 0.6280\n",
      "Epoch 15/30, Validation Loss: 0.6247\n",
      "Epoch 16/30, Validation Loss: 0.6250\n",
      "Early stopping triggered.\n",
      "Epoch 1/70, Validation Loss: 0.6216\n",
      "Epoch 2/70, Validation Loss: 0.6149\n",
      "Epoch 3/70, Validation Loss: 0.6129\n",
      "Epoch 4/70, Validation Loss: 0.6094\n",
      "Epoch 5/70, Validation Loss: 0.6093\n",
      "Epoch 6/70, Validation Loss: 0.6076\n",
      "Epoch 7/70, Validation Loss: 0.6068\n",
      "Epoch 8/70, Validation Loss: 0.6105\n",
      "Epoch 9/70, Validation Loss: 0.6160\n",
      "Epoch 10/70, Validation Loss: 0.6178\n",
      "Epoch 11/70, Validation Loss: 0.6240\n",
      "Epoch 12/70, Validation Loss: 0.6237\n",
      "Early stopping triggered.\n",
      "✅ Final fine-tuned model saved at: C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/Models/American_to_African/MobileNetV2_ft_final_model.pth\n",
      "✅ Test Accuracy after Fine-Tuning: 0.3203\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "FROZEN_EPOCHS = 30\n",
    "FINE_TUNE_EPOCHS = 70\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "FINE_TUNE_LR = 1e-5\n",
    "BEST_MODEL_PATH = 'C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/Models/American_to_African/MobileNetV2_ft_best_model.pth'\n",
    "FINAL_MODEL_PATH = 'C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/Models/American_to_African/MobileNetV2_ft_final_model.pth'\n",
    "\n",
    "# Transforms\n",
    "transform = Compose([\n",
    "    Resize((224, 224)),  # MobileNetV1 typically uses 224x224 images\n",
    "    ToTensor(),\n",
    "    Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset & Dataloaders (Updated paths)\n",
    "train_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/DatasetAmericantoAfrican/train', transform=transform)\n",
    "val_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/DatasetAmericantoAfrican/valid', transform=transform)\n",
    "test_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/DatasetAmericantoAfrican/test', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Load pretrained MobileNetV1 and freeze feature layers\n",
    "base_model = models.mobilenet_v2(pretrained=True)  # MobileNetV1 equivalent is available as MobileNetV2 in torchvision\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False  # Freeze all layers initially\n",
    "\n",
    "# Replace the classifier part of the model (keep global average pooling)\n",
    "base_model.classifier = nn.Sequential(\n",
    "    nn.Linear(base_model.classifier[1].in_features, 512),  # base_model.classifier[1].in_features gives the correct input size\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(512, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "model = base_model.to(device)\n",
    "\n",
    "# Loss & optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "\n",
    "# Early Stopping Class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, path=BEST_MODEL_PATH):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.best_model = None\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            torch.save(self.best_model, self.path)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        return self.counter >= self.patience\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "# Accuracy Function\n",
    "def compute_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = (outputs > 0.5).int().squeeze()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs):\n",
    "    early_stopping = EarlyStopping(patience=5)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        val_loss = evaluate(model, val_loader, criterion)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if early_stopping(val_loss, model):\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Train model with frozen layers\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, FROZEN_EPOCHS)\n",
    "\n",
    "# Load best model before fine-tuning\n",
    "model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "\n",
    "# Unfreeze all layers in the classifier and some feature layers\n",
    "for param in list(model.features.parameters())[-50:]:\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Recompile optimizer with updated params\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=FINE_TUNE_LR)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "\n",
    "# Fine-tune the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, FINE_TUNE_EPOCHS)\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), FINAL_MODEL_PATH)\n",
    "print(f\"✅ Final fine-tuned model saved at: {FINAL_MODEL_PATH}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_acc = compute_accuracy(model, test_loader)\n",
    "print(f\"✅ Test Accuracy after Fine-Tuning: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 MobileNet v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Validation Loss: 0.6655\n",
      "Epoch 2/30, Validation Loss: 0.6499\n",
      "Epoch 3/30, Validation Loss: 0.6420\n",
      "Epoch 4/30, Validation Loss: 0.6444\n",
      "Epoch 5/30, Validation Loss: 0.6386\n",
      "Epoch 6/30, Validation Loss: 0.6342\n",
      "Epoch 7/30, Validation Loss: 0.6510\n",
      "Epoch 8/30, Validation Loss: 0.6304\n",
      "Epoch 9/30, Validation Loss: 0.6466\n",
      "Epoch 10/30, Validation Loss: 0.6252\n",
      "Epoch 11/30, Validation Loss: 0.6286\n",
      "Epoch 12/30, Validation Loss: 0.6257\n",
      "Epoch 13/30, Validation Loss: 0.6285\n",
      "Epoch 14/30, Validation Loss: 0.6235\n",
      "Epoch 15/30, Validation Loss: 0.6297\n",
      "Epoch 16/30, Validation Loss: 0.6482\n",
      "Epoch 17/30, Validation Loss: 0.6249\n",
      "Epoch 18/30, Validation Loss: 0.6145\n",
      "Epoch 19/30, Validation Loss: 0.6149\n",
      "Epoch 20/30, Validation Loss: 0.6136\n",
      "Epoch 21/30, Validation Loss: 0.6186\n",
      "Epoch 22/30, Validation Loss: 0.6115\n",
      "Epoch 23/30, Validation Loss: 0.6113\n",
      "Epoch 24/30, Validation Loss: 0.6105\n",
      "Epoch 25/30, Validation Loss: 0.6082\n",
      "Epoch 26/30, Validation Loss: 0.6037\n",
      "Epoch 27/30, Validation Loss: 0.6259\n",
      "Epoch 28/30, Validation Loss: 0.5995\n",
      "Epoch 29/30, Validation Loss: 0.6000\n",
      "Epoch 30/30, Validation Loss: 0.5978\n",
      "Epoch 1/70, Validation Loss: 0.5915\n",
      "Epoch 2/70, Validation Loss: 0.5884\n",
      "Epoch 3/70, Validation Loss: 0.5851\n",
      "Epoch 4/70, Validation Loss: 0.5799\n",
      "Epoch 5/70, Validation Loss: 0.5744\n",
      "Epoch 6/70, Validation Loss: 0.5768\n",
      "Epoch 7/70, Validation Loss: 0.5712\n",
      "Epoch 8/70, Validation Loss: 0.5703\n",
      "Epoch 9/70, Validation Loss: 0.5700\n",
      "Epoch 10/70, Validation Loss: 0.5665\n",
      "Epoch 11/70, Validation Loss: 0.5690\n",
      "Epoch 12/70, Validation Loss: 0.5677\n",
      "Epoch 13/70, Validation Loss: 0.5683\n",
      "Epoch 14/70, Validation Loss: 0.5670\n",
      "Epoch 15/70, Validation Loss: 0.5668\n",
      "Early stopping triggered.\n",
      "✅ Final fine-tuned model saved at: C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/Models/American_to_African/MobileNetV3_ft_final_model.pth\n",
      "✅ Test Accuracy after Fine-Tuning: 0.4775\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "FROZEN_EPOCHS = 30\n",
    "FINE_TUNE_EPOCHS = 70\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "FINE_TUNE_LR = 1e-5\n",
    "BEST_MODEL_PATH = 'C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/Models/American_to_African/MobileNetV3_ft_best_model.pth'\n",
    "FINAL_MODEL_PATH = 'C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/Models/American_to_African/MobileNetV3_ft_final_model.pth'\n",
    "\n",
    "# Transforms\n",
    "transform = Compose([\n",
    "    Resize((224, 224)),  # MobileNetV3 typically uses 224x224 images\n",
    "    ToTensor(),\n",
    "    Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset & Dataloaders (Updated paths)\n",
    "train_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/DatasetAmericantoAfrican/train', transform=transform)\n",
    "val_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/DatasetAmericantoAfrican/valid', transform=transform)\n",
    "test_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/DatasetAmericantoAfrican/test', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Load pretrained MobileNetV3 and freeze feature layers\n",
    "base_model = models.mobilenet_v3_large(pretrained=True)  # MobileNetV3\n",
    "base_model = base_model.to(device)  # Move the model to the GPU (if available)\n",
    "\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False  # Freeze all layers initially\n",
    "\n",
    "# Dummy input to get the number of input features to the classifier\n",
    "dummy_input = torch.randn(1, 3, 224, 224).to(device)  # Example input with the same size as the input images\n",
    "feature_output = base_model.features(dummy_input)  # Get output from the feature extractor\n",
    "feature_output = base_model.avgpool(feature_output)  # Apply average pooling\n",
    "feature_output = torch.flatten(feature_output, 1)  # Flatten the pooled output\n",
    "num_ftrs = feature_output.size(1)  # The number of features (flattened size)\n",
    "\n",
    "# Replace the classifier part of the model (keep global average pooling)\n",
    "base_model.classifier = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 512),  # Use num_ftrs for the input size of the linear layer\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(512, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "model = base_model.to(device)  # Ensure the model is on the same device\n",
    "\n",
    "# Loss & optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "\n",
    "# Early Stopping Class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, path=BEST_MODEL_PATH):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.best_model = None\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            torch.save(self.best_model, self.path)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        return self.counter >= self.patience\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "# Accuracy Function\n",
    "def compute_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = (outputs > 0.5).int().squeeze()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs):\n",
    "    early_stopping = EarlyStopping(patience=5)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        val_loss = evaluate(model, val_loader, criterion)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if early_stopping(val_loss, model):\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Train model with frozen layers\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, FROZEN_EPOCHS)\n",
    "\n",
    "# Load best model before fine-tuning\n",
    "model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "\n",
    "# Unfreeze all layers in the classifier and some feature layers\n",
    "for param in list(model.features.parameters())[-50:]:\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Recompile optimizer with updated params\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=FINE_TUNE_LR)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "\n",
    "# Fine-tune the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, FINE_TUNE_EPOCHS)\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), FINAL_MODEL_PATH)\n",
    "print(f\"✅ Final fine-tuned model saved at: {FINAL_MODEL_PATH}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_acc = compute_accuracy(model, test_loader)\n",
    "print(f\"✅ Test Accuracy after Fine-Tuning: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 DenseNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 DenseNet 121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Validation Loss: 0.6567\n",
      "Epoch 2/30, Validation Loss: 0.6481\n",
      "Epoch 3/30, Validation Loss: 0.6427\n",
      "Epoch 4/30, Validation Loss: 0.6387\n",
      "Epoch 5/30, Validation Loss: 0.6368\n",
      "Epoch 6/30, Validation Loss: 0.6306\n",
      "Epoch 7/30, Validation Loss: 0.6298\n",
      "Epoch 8/30, Validation Loss: 0.6470\n",
      "Epoch 9/30, Validation Loss: 0.6324\n",
      "Epoch 10/30, Validation Loss: 0.6246\n",
      "Epoch 11/30, Validation Loss: 0.6305\n",
      "Epoch 12/30, Validation Loss: 0.6250\n",
      "Epoch 13/30, Validation Loss: 0.6273\n",
      "Epoch 14/30, Validation Loss: 0.6284\n",
      "Epoch 15/30, Validation Loss: 0.6188\n",
      "Epoch 16/30, Validation Loss: 0.6189\n",
      "Epoch 17/30, Validation Loss: 0.6188\n",
      "Epoch 18/30, Validation Loss: 0.6169\n",
      "Epoch 19/30, Validation Loss: 0.6159\n",
      "Epoch 20/30, Validation Loss: 0.6203\n",
      "Epoch 21/30, Validation Loss: 0.6179\n",
      "Epoch 22/30, Validation Loss: 0.6201\n",
      "Epoch 23/30, Validation Loss: 0.6172\n",
      "Epoch 24/30, Validation Loss: 0.6178\n",
      "Early stopping triggered.\n",
      "Epoch 1/70, Validation Loss: 0.6155\n",
      "Epoch 2/70, Validation Loss: 0.6159\n",
      "Epoch 3/70, Validation Loss: 0.6119\n",
      "Epoch 4/70, Validation Loss: 0.6076\n",
      "Epoch 5/70, Validation Loss: 0.6071\n",
      "Epoch 6/70, Validation Loss: 0.6086\n",
      "Epoch 7/70, Validation Loss: 0.6050\n",
      "Epoch 8/70, Validation Loss: 0.6028\n",
      "Epoch 9/70, Validation Loss: 0.6023\n",
      "Epoch 10/70, Validation Loss: 0.5968\n",
      "Epoch 11/70, Validation Loss: 0.5982\n",
      "Epoch 12/70, Validation Loss: 0.5960\n",
      "Epoch 13/70, Validation Loss: 0.5946\n",
      "Epoch 14/70, Validation Loss: 0.5967\n",
      "Epoch 15/70, Validation Loss: 0.5933\n",
      "Epoch 16/70, Validation Loss: 0.5911\n",
      "Epoch 17/70, Validation Loss: 0.5910\n",
      "Epoch 18/70, Validation Loss: 0.5872\n",
      "Epoch 19/70, Validation Loss: 0.5898\n",
      "Epoch 20/70, Validation Loss: 0.5884\n",
      "Epoch 21/70, Validation Loss: 0.5894\n",
      "Epoch 22/70, Validation Loss: 0.5896\n",
      "Epoch 23/70, Validation Loss: 0.5872\n",
      "Epoch 24/70, Validation Loss: 0.5824\n",
      "Epoch 25/70, Validation Loss: 0.5899\n",
      "Epoch 26/70, Validation Loss: 0.5836\n",
      "Epoch 27/70, Validation Loss: 0.5846\n",
      "Epoch 28/70, Validation Loss: 0.5864\n",
      "Epoch 29/70, Validation Loss: 0.5869\n",
      "Early stopping triggered.\n",
      "✅ Final fine-tuned model saved at: C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/Models/American_to_African/DenseNet121_ft_final_model.pth\n",
      "✅ Test Accuracy after Fine-Tuning: 0.2896\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "FROZEN_EPOCHS = 30\n",
    "FINE_TUNE_EPOCHS = 70\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "FINE_TUNE_LR = 1e-5\n",
    "BEST_MODEL_PATH = 'C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/Models/American_to_African/DenseNet121_ft_best_model.pth'\n",
    "FINAL_MODEL_PATH = 'C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/Models/American_to_African/DenseNet121_ft_final_model.pth'\n",
    "\n",
    "# Transforms\n",
    "transform = Compose([\n",
    "    Resize((224, 224)),  # DenseNet121 typically uses 224x224 images\n",
    "    ToTensor(),\n",
    "    Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset & Dataloaders (Updated paths)\n",
    "train_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/DatasetAmericantoAfrican/train', transform=transform)\n",
    "val_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/DatasetAmericantoAfrican/valid', transform=transform)\n",
    "test_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/DatasetAmericantoAfrican/test', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Load pretrained DenseNet121 and freeze feature layers\n",
    "base_model = models.densenet121(pretrained=True)  # DenseNet121\n",
    "base_model = base_model.to(device)  # Ensure the model is on the correct device\n",
    "\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False  # Freeze all layers initially\n",
    "\n",
    "# Get the correct number of features from the classifier layer\n",
    "num_ftrs = base_model.classifier.in_features  # Access the input features to the first Linear layer\n",
    "\n",
    "# Replace the classifier part of the model (keep global average pooling)\n",
    "base_model.classifier = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 512),  # Use num_ftrs for the input size of the linear layer\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(512, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "model = base_model.to(device)  # Ensure the model is on the correct device\n",
    "\n",
    "# Loss & optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "\n",
    "# Early Stopping Class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, path=BEST_MODEL_PATH):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.best_model = None\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            torch.save(self.best_model, self.path)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        return self.counter >= self.patience\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "# Accuracy Function\n",
    "def compute_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = (outputs > 0.5).int().squeeze()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs):\n",
    "    early_stopping = EarlyStopping(patience=5)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        val_loss = evaluate(model, val_loader, criterion)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if early_stopping(val_loss, model):\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Train model with frozen layers\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, FROZEN_EPOCHS)\n",
    "\n",
    "# Load best model before fine-tuning\n",
    "model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "\n",
    "# Unfreeze all layers in the classifier and some feature layers\n",
    "for param in list(model.features.parameters())[-50:]:\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Recompile optimizer with updated params\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=FINE_TUNE_LR)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "\n",
    "# Fine-tune the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, FINE_TUNE_EPOCHS)\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), FINAL_MODEL_PATH)\n",
    "print(f\"✅ Final fine-tuned model saved at: {FINAL_MODEL_PATH}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_acc = compute_accuracy(model, test_loader)\n",
    "print(f\"✅ Test Accuracy after Fine-Tuning: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 DenseNet 169"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vb11574\\AppData\\Local\\miniconda3\\envs\\salmo\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\vb11574\\AppData\\Local\\miniconda3\\envs\\salmo\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet169_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet169_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/densenet169-b2777c0a.pth\" to C:\\Users\\vb11574/.cache\\torch\\hub\\checkpoints\\densenet169-b2777c0a.pth\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Validation Loss: 0.6566\n",
      "Epoch 2/30, Validation Loss: 0.6542\n",
      "Epoch 3/30, Validation Loss: 0.6475\n",
      "Epoch 4/30, Validation Loss: 0.6822\n",
      "Epoch 5/30, Validation Loss: 0.6263\n",
      "Epoch 6/30, Validation Loss: 0.6223\n",
      "Epoch 7/30, Validation Loss: 0.6179\n",
      "Epoch 8/30, Validation Loss: 0.6226\n",
      "Epoch 9/30, Validation Loss: 0.6155\n",
      "Epoch 10/30, Validation Loss: 0.6109\n",
      "Epoch 11/30, Validation Loss: 0.6439\n",
      "Epoch 12/30, Validation Loss: 0.6076\n",
      "Epoch 13/30, Validation Loss: 0.6067\n",
      "Epoch 14/30, Validation Loss: 0.6093\n",
      "Epoch 15/30, Validation Loss: 0.6161\n",
      "Epoch 16/30, Validation Loss: 0.6058\n",
      "Epoch 17/30, Validation Loss: 0.6142\n",
      "Epoch 18/30, Validation Loss: 0.6139\n",
      "Epoch 19/30, Validation Loss: 0.5999\n",
      "Epoch 20/30, Validation Loss: 0.5994\n",
      "Epoch 21/30, Validation Loss: 0.5969\n",
      "Epoch 22/30, Validation Loss: 0.5994\n",
      "Epoch 23/30, Validation Loss: 0.6219\n",
      "Epoch 24/30, Validation Loss: 0.5974\n",
      "Epoch 25/30, Validation Loss: 0.6004\n",
      "Epoch 26/30, Validation Loss: 0.6021\n",
      "Early stopping triggered.\n",
      "Epoch 1/70, Validation Loss: 0.5972\n",
      "Epoch 2/70, Validation Loss: 0.5992\n",
      "Epoch 3/70, Validation Loss: 0.5951\n",
      "Epoch 4/70, Validation Loss: 0.5932\n",
      "Epoch 5/70, Validation Loss: 0.5942\n",
      "Epoch 6/70, Validation Loss: 0.5912\n",
      "Epoch 7/70, Validation Loss: 0.5908\n",
      "Epoch 8/70, Validation Loss: 0.5936\n",
      "Epoch 9/70, Validation Loss: 0.5914\n",
      "Epoch 10/70, Validation Loss: 0.5906\n",
      "Epoch 11/70, Validation Loss: 0.5877\n",
      "Epoch 12/70, Validation Loss: 0.5881\n",
      "Epoch 13/70, Validation Loss: 0.5863\n",
      "Epoch 14/70, Validation Loss: 0.5884\n",
      "Epoch 15/70, Validation Loss: 0.5838\n",
      "Epoch 16/70, Validation Loss: 0.5859\n",
      "Epoch 17/70, Validation Loss: 0.5837\n",
      "Epoch 18/70, Validation Loss: 0.5861\n",
      "Epoch 19/70, Validation Loss: 0.5853\n",
      "Epoch 20/70, Validation Loss: 0.5898\n",
      "Epoch 21/70, Validation Loss: 0.5839\n",
      "Epoch 22/70, Validation Loss: 0.5846\n",
      "Early stopping triggered.\n",
      "✅ Final fine-tuned model saved at: C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/Models/American_to_African/DenseNet121_ft_final_model.pth\n",
      "✅ Test Accuracy after Fine-Tuning: 0.5846\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "FROZEN_EPOCHS = 30\n",
    "FINE_TUNE_EPOCHS = 70\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "FINE_TUNE_LR = 1e-5\n",
    "BEST_MODEL_PATH = 'C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/Models/American_to_African/DenseNet121_ft_best_model.pth'\n",
    "FINAL_MODEL_PATH = 'C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/Models/American_to_African/DenseNet121_ft_final_model.pth'\n",
    "\n",
    "# Transforms\n",
    "transform = Compose([\n",
    "    Resize((224, 224)),  # DenseNet121 typically uses 224x224 images\n",
    "    ToTensor(),\n",
    "    Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset & Dataloaders (Updated paths)\n",
    "train_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/DatasetAmericantoAfrican/train', transform=transform)\n",
    "val_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/DatasetAmericantoAfrican/valid', transform=transform)\n",
    "test_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/DatasetAmericantoAfrican/test', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Load pretrained DenseNet121 and freeze feature layers\n",
    "base_model = models.densenet169(pretrained=True)  # DenseNet121\n",
    "base_model = base_model.to(device)  # Ensure the model is on the correct device\n",
    "\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False  # Freeze all layers initially\n",
    "\n",
    "# Get the correct number of features from the classifier layer\n",
    "num_ftrs = base_model.classifier.in_features  # Access the input features to the first Linear layer\n",
    "\n",
    "# Replace the classifier part of the model (keep global average pooling)\n",
    "base_model.classifier = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 512),  # Use num_ftrs for the input size of the linear layer\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(512, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "model = base_model.to(device)  # Ensure the model is on the correct device\n",
    "\n",
    "# Loss & optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "\n",
    "# Early Stopping Class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, path=BEST_MODEL_PATH):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.best_model = None\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            torch.save(self.best_model, self.path)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        return self.counter >= self.patience\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "# Accuracy Function\n",
    "def compute_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = (outputs > 0.5).int().squeeze()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs):\n",
    "    early_stopping = EarlyStopping(patience=5)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        val_loss = evaluate(model, val_loader, criterion)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if early_stopping(val_loss, model):\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Train model with frozen layers\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, FROZEN_EPOCHS)\n",
    "\n",
    "# Load best model before fine-tuning\n",
    "model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "\n",
    "# Unfreeze all layers in the classifier and some feature layers\n",
    "for param in list(model.features.parameters())[-50:]:\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Recompile optimizer with updated params\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=FINE_TUNE_LR)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "\n",
    "# Fine-tune the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, FINE_TUNE_EPOCHS)\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), FINAL_MODEL_PATH)\n",
    "print(f\"✅ Final fine-tuned model saved at: {FINAL_MODEL_PATH}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_acc = compute_accuracy(model, test_loader)\n",
    "print(f\"✅ Test Accuracy after Fine-Tuning: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 DenseNet 201"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vb11574\\AppData\\Local\\miniconda3\\envs\\salmo\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet201_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet201_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/densenet201-c1103571.pth\" to C:\\Users\\vb11574/.cache\\torch\\hub\\checkpoints\\densenet201-c1103571.pth\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Validation Loss: 0.6587\n",
      "Epoch 2/30, Validation Loss: 0.6500\n",
      "Epoch 3/30, Validation Loss: 0.6431\n",
      "Epoch 4/30, Validation Loss: 0.6365\n",
      "Epoch 5/30, Validation Loss: 0.6342\n",
      "Epoch 6/30, Validation Loss: 0.6403\n",
      "Epoch 7/30, Validation Loss: 0.6287\n",
      "Epoch 8/30, Validation Loss: 0.6260\n",
      "Epoch 9/30, Validation Loss: 0.6242\n",
      "Epoch 10/30, Validation Loss: 0.6418\n",
      "Epoch 11/30, Validation Loss: 0.6280\n",
      "Epoch 12/30, Validation Loss: 0.6246\n",
      "Epoch 13/30, Validation Loss: 0.6267\n",
      "Epoch 14/30, Validation Loss: 0.6241\n",
      "Epoch 15/30, Validation Loss: 0.6244\n",
      "Epoch 16/30, Validation Loss: 0.6255\n",
      "Epoch 17/30, Validation Loss: 0.6235\n",
      "Epoch 18/30, Validation Loss: 0.6222\n",
      "Epoch 19/30, Validation Loss: 0.6226\n",
      "Epoch 20/30, Validation Loss: 0.6230\n",
      "Epoch 21/30, Validation Loss: 0.6245\n",
      "Epoch 22/30, Validation Loss: 0.6250\n",
      "Epoch 23/30, Validation Loss: 0.6247\n",
      "Early stopping triggered.\n",
      "Epoch 1/70, Validation Loss: 0.6217\n",
      "Epoch 2/70, Validation Loss: 0.6187\n",
      "Epoch 3/70, Validation Loss: 0.6192\n",
      "Epoch 4/70, Validation Loss: 0.6124\n",
      "Epoch 5/70, Validation Loss: 0.6109\n",
      "Epoch 6/70, Validation Loss: 0.6094\n",
      "Epoch 7/70, Validation Loss: 0.6079\n",
      "Epoch 8/70, Validation Loss: 0.6069\n",
      "Epoch 9/70, Validation Loss: 0.6032\n",
      "Epoch 10/70, Validation Loss: 0.6035\n",
      "Epoch 11/70, Validation Loss: 0.5968\n",
      "Epoch 12/70, Validation Loss: 0.5929\n",
      "Epoch 13/70, Validation Loss: 0.5936\n",
      "Epoch 14/70, Validation Loss: 0.5931\n",
      "Epoch 15/70, Validation Loss: 0.5890\n",
      "Epoch 16/70, Validation Loss: 0.5808\n",
      "Epoch 17/70, Validation Loss: 0.5793\n",
      "Epoch 18/70, Validation Loss: 0.5755\n",
      "Epoch 19/70, Validation Loss: 0.5775\n",
      "Epoch 20/70, Validation Loss: 0.5750\n",
      "Epoch 21/70, Validation Loss: 0.5815\n",
      "Epoch 22/70, Validation Loss: 0.5713\n",
      "Epoch 23/70, Validation Loss: 0.5664\n",
      "Epoch 24/70, Validation Loss: 0.5699\n",
      "Epoch 25/70, Validation Loss: 0.5650\n",
      "Epoch 26/70, Validation Loss: 0.5668\n",
      "Epoch 27/70, Validation Loss: 0.5641\n",
      "Epoch 28/70, Validation Loss: 0.5674\n",
      "Epoch 29/70, Validation Loss: 0.5687\n",
      "Epoch 30/70, Validation Loss: 0.5737\n",
      "Epoch 31/70, Validation Loss: 0.5711\n",
      "Epoch 32/70, Validation Loss: 0.5641\n",
      "Epoch 33/70, Validation Loss: 0.5684\n",
      "Epoch 34/70, Validation Loss: 0.5760\n",
      "Epoch 35/70, Validation Loss: 0.5787\n",
      "Epoch 36/70, Validation Loss: 0.5708\n",
      "Epoch 37/70, Validation Loss: 0.5714\n",
      "Early stopping triggered.\n",
      "✅ Final fine-tuned model saved at: C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/Models/American_to_African/DenseNet201_ft_final_model.pth\n",
      "✅ Test Accuracy after Fine-Tuning: 0.3944\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "FROZEN_EPOCHS = 30\n",
    "FINE_TUNE_EPOCHS = 70\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "FINE_TUNE_LR = 1e-5\n",
    "BEST_MODEL_PATH = 'C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/Models/American_to_African/DenseNet201_ft_best_model.pth'\n",
    "FINAL_MODEL_PATH = 'C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/Models/American_to_African/DenseNet201_ft_final_model.pth'\n",
    "\n",
    "# Transforms\n",
    "transform = Compose([\n",
    "    Resize((224, 224)),  # MobileNetV1 typically uses 224x224 images\n",
    "    ToTensor(),\n",
    "    Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset & Dataloaders (Updated paths)\n",
    "train_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/DatasetAmericantoAfrican/train', transform=transform)\n",
    "val_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/DatasetAmericantoAfrican/valid', transform=transform)\n",
    "test_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/DatasetAmericantoAfrican/test', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Load pretrained MobileNetV1 and freeze feature layers\n",
    "base_model = models.densenet201(pretrained=True)  # MobileNetV1 equivalent is available as MobileNetV2 in torchvision\n",
    "base_model = base_model.to(device)  # Ensure the model is on the correct device\n",
    "\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False  # Freeze all layers initially\n",
    "\n",
    "# Get the correct number of features from the classifier layer\n",
    "num_ftrs = base_model.classifier.in_features  # Access the input features to the first Linear layer\n",
    "\n",
    "# Replace the classifier part of the model (keep global average pooling)\n",
    "base_model.classifier = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 512),  # base_model.classifier[1].in_features gives the correct input size\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(512, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "model = base_model.to(device)\n",
    "\n",
    "# Loss & optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "\n",
    "# Early Stopping Class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, path=BEST_MODEL_PATH):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.best_model = None\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            torch.save(self.best_model, self.path)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        return self.counter >= self.patience\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "# Accuracy Function\n",
    "def compute_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = (outputs > 0.5).int().squeeze()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs):\n",
    "    early_stopping = EarlyStopping(patience=5)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        val_loss = evaluate(model, val_loader, criterion)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if early_stopping(val_loss, model):\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Train model with frozen layers\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, FROZEN_EPOCHS)\n",
    "\n",
    "# Load best model before fine-tuning\n",
    "model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "\n",
    "# Unfreeze all layers in the classifier and some feature layers\n",
    "for param in list(model.features.parameters())[-50:]:\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Recompile optimizer with updated params\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=FINE_TUNE_LR)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "\n",
    "# Fine-tune the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, FINE_TUNE_EPOCHS)\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), FINAL_MODEL_PATH)\n",
    "print(f\"✅ Final fine-tuned model saved at: {FINAL_MODEL_PATH}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_acc = compute_accuracy(model, test_loader)\n",
    "print(f\"✅ Test Accuracy after Fine-Tuning: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Inception V3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vb11574\\AppData\\Local\\miniconda3\\envs\\salmo\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to C:\\Users\\vb11574/.cache\\torch\\hub\\checkpoints\\inception_v3_google-0cc3c7bd.pth\n",
      "100.0%\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Inception3' object has no attribute 'classifier'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     42\u001b[39m     param.requires_grad = \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Freeze all layers initially\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# Replace the classifier part of the model (keep global average pooling)\u001b[39;00m\n\u001b[32m     45\u001b[39m base_model.classifier = nn.Sequential(\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     nn.Linear(\u001b[43mbase_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclassifier\u001b[49m[\u001b[32m1\u001b[39m].in_features, \u001b[32m512\u001b[39m),  \u001b[38;5;66;03m# base_model.classifier[1].in_features gives the correct input size\u001b[39;00m\n\u001b[32m     47\u001b[39m     nn.ReLU(),\n\u001b[32m     48\u001b[39m     nn.Dropout(\u001b[32m0.5\u001b[39m),\n\u001b[32m     49\u001b[39m     nn.Linear(\u001b[32m512\u001b[39m, \u001b[32m1\u001b[39m),\n\u001b[32m     50\u001b[39m     nn.Sigmoid()\n\u001b[32m     51\u001b[39m )\n\u001b[32m     53\u001b[39m model = base_model.to(device)\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Loss & optimizer\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vb11574\\AppData\\Local\\miniconda3\\envs\\salmo\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1928\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1926\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1927\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1928\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1929\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1930\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'Inception3' object has no attribute 'classifier'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "FROZEN_EPOCHS = 30\n",
    "FINE_TUNE_EPOCHS = 70\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "FINE_TUNE_LR = 1e-5\n",
    "BEST_MODEL_PATH = 'C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/Models/American_to_African/InceptionV3_ft_best_model.pth'\n",
    "FINAL_MODEL_PATH = 'C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/Models/American_to_African/InceptionV3_ft_final_model.pth'\n",
    "\n",
    "# Transforms\n",
    "transform = Compose([\n",
    "    Resize((224, 224)),  # MobileNetV1 typically uses 224x224 images\n",
    "    ToTensor(),\n",
    "    Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset & Dataloaders (Updated paths)\n",
    "train_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/DatasetAmericantoAfrican/train', transform=transform)\n",
    "val_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/DatasetAmericantoAfrican/valid', transform=transform)\n",
    "test_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/DatasetAmericantoAfrican/test', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Load pretrained MobileNetV1 and freeze feature layers\n",
    "base_model = models.inception_v3(pretrained=True)  # MobileNetV1 equivalent is available as MobileNetV2 in torchvision\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False  # Freeze all layers initially\n",
    "\n",
    "# Replace the classifier part of the model (keep global average pooling)\n",
    "base_model.classifier = nn.Sequential(\n",
    "    nn.Linear(base_model.classifier[1].in_features, 512),  # base_model.classifier[1].in_features gives the correct input size\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(512, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "model = base_model.to(device)\n",
    "\n",
    "# Loss & optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "\n",
    "# Early Stopping Class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, path=BEST_MODEL_PATH):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.best_model = None\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            torch.save(self.best_model, self.path)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        return self.counter >= self.patience\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "# Accuracy Function\n",
    "def compute_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = (outputs > 0.5).int().squeeze()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs):\n",
    "    early_stopping = EarlyStopping(patience=5)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        val_loss = evaluate(model, val_loader, criterion)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if early_stopping(val_loss, model):\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Train model with frozen layers\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, FROZEN_EPOCHS)\n",
    "\n",
    "# Load best model before fine-tuning\n",
    "model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "\n",
    "# Unfreeze all layers in the classifier and some feature layers\n",
    "for param in list(model.features.parameters())[-50:]:\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Recompile optimizer with updated params\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=FINE_TUNE_LR)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "\n",
    "# Fine-tune the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, FINE_TUNE_EPOCHS)\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), FINAL_MODEL_PATH)\n",
    "print(f\"✅ Final fine-tuned model saved at: {FINAL_MODEL_PATH}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_acc = compute_accuracy(model, test_loader)\n",
    "print(f\"✅ Test Accuracy after Fine-Tuning: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 ResNet 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Validation Loss: 0.6597\n",
      "Epoch 2/30, Validation Loss: 0.6553\n",
      "Epoch 3/30, Validation Loss: 0.6472\n",
      "Epoch 4/30, Validation Loss: 0.6429\n",
      "Epoch 5/30, Validation Loss: 0.6403\n",
      "Epoch 6/30, Validation Loss: 0.6420\n",
      "Epoch 7/30, Validation Loss: 0.6388\n",
      "Epoch 8/30, Validation Loss: 0.6366\n",
      "Epoch 9/30, Validation Loss: 0.6369\n",
      "Epoch 10/30, Validation Loss: 0.6358\n",
      "Epoch 11/30, Validation Loss: 0.6391\n",
      "Epoch 12/30, Validation Loss: 0.6390\n",
      "Epoch 13/30, Validation Loss: 0.6408\n",
      "Epoch 14/30, Validation Loss: 0.6449\n",
      "Epoch 15/30, Validation Loss: 0.6350\n",
      "Epoch 16/30, Validation Loss: 0.6367\n",
      "Epoch 17/30, Validation Loss: 0.6345\n",
      "Epoch 18/30, Validation Loss: 0.6366\n",
      "Epoch 19/30, Validation Loss: 0.6349\n",
      "Epoch 20/30, Validation Loss: 0.6362\n",
      "Epoch 21/30, Validation Loss: 0.6379\n",
      "Epoch 22/30, Validation Loss: 0.6351\n",
      "Early stopping triggered.\n",
      "Epoch 1/70, Validation Loss: 0.6164\n",
      "Epoch 2/70, Validation Loss: 0.6097\n",
      "Epoch 3/70, Validation Loss: 0.6108\n",
      "Epoch 4/70, Validation Loss: 0.6243\n",
      "Epoch 5/70, Validation Loss: 0.6506\n",
      "Epoch 6/70, Validation Loss: 0.6946\n",
      "Epoch 7/70, Validation Loss: 0.6928\n",
      "Early stopping triggered.\n",
      "✅ Final fine-tuned model saved at: C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/Models/American_to_African/ResNet50_ft_final_model.pth\n",
      "✅ Test Accuracy after Fine-Tuning: 0.5140\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "FROZEN_EPOCHS = 30\n",
    "FINE_TUNE_EPOCHS = 70\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "FINE_TUNE_LR = 1e-5\n",
    "BEST_MODEL_PATH = 'C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/Models/American_to_African/ResNet50_ft_best_model.pth'\n",
    "FINAL_MODEL_PATH = 'C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/Models/American_to_African/ResNet50_ft_final_model.pth'\n",
    "\n",
    "# Transforms\n",
    "transform = Compose([\n",
    "    Resize((224, 224)),  # ResNet50 typically uses 224x224 images\n",
    "    ToTensor(),\n",
    "    Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset & Dataloaders (Updated paths)\n",
    "train_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/DatasetAmericantoAfrican/train', transform=transform)\n",
    "val_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/DatasetAmericantoAfrican/valid', transform=transform)\n",
    "test_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/DatasetAmericantoAfrican/test', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Load pretrained ResNet50 and freeze feature layers\n",
    "base_model = models.resnet50(pretrained=True)  # ResNet50\n",
    "base_model = base_model.to(device)  # Ensure the model is on the correct device\n",
    "\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False  # Freeze all layers initially\n",
    "\n",
    "# Get the correct number of features from the fc layer (Fully connected layer)\n",
    "num_ftrs = base_model.fc.in_features  # Get input features to the first Linear layer (fc layer)\n",
    "\n",
    "# Replace the classifier part of the model (fully connected layer)\n",
    "base_model.fc = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 512),  # Use num_ftrs for the input size of the linear layer\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(512, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "model = base_model.to(device)  # Ensure the model is on the correct device\n",
    "\n",
    "# Loss & optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=LEARNING_RATE)  # Update optimizer to work with the new classifier\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "\n",
    "# Early Stopping Class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, path=BEST_MODEL_PATH):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.best_model = None\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            torch.save(self.best_model, self.path)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        return self.counter >= self.patience\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "# Accuracy Function\n",
    "def compute_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = (outputs > 0.5).int().squeeze()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs):\n",
    "    early_stopping = EarlyStopping(patience=5)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        val_loss = evaluate(model, val_loader, criterion)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if early_stopping(val_loss, model):\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Train model with frozen layers\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, FROZEN_EPOCHS)\n",
    "\n",
    "# Load best model before fine-tuning\n",
    "model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "\n",
    "# Unfreeze all layers in the classifier and some feature layers\n",
    "for param in list(model.parameters())[-50:]:\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Recompile optimizer with updated params\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=FINE_TUNE_LR)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "\n",
    "# Fine-tune the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, FINE_TUNE_EPOCHS)\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), FINAL_MODEL_PATH)\n",
    "print(f\"✅ Final fine-tuned model saved at: {FINAL_MODEL_PATH}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_acc = compute_accuracy(model, test_loader)\n",
    "print(f\"✅ Test Accuracy after Fine-Tuning: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 ResNet 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vb11574\\AppData\\Local\\miniconda3\\envs\\salmo\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\vb11574\\AppData\\Local\\miniconda3\\envs\\salmo\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet101_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet101_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet101-63fe2227.pth\" to C:\\Users\\vb11574/.cache\\torch\\hub\\checkpoints\\resnet101-63fe2227.pth\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Validation Loss: 0.6752\n",
      "Epoch 2/30, Validation Loss: 0.6499\n",
      "Epoch 3/30, Validation Loss: 0.6462\n",
      "Epoch 4/30, Validation Loss: 0.6477\n",
      "Epoch 5/30, Validation Loss: 0.6412\n",
      "Epoch 6/30, Validation Loss: 0.6438\n",
      "Epoch 7/30, Validation Loss: 0.6376\n",
      "Epoch 8/30, Validation Loss: 0.6396\n",
      "Epoch 9/30, Validation Loss: 0.6311\n",
      "Epoch 10/30, Validation Loss: 0.6293\n",
      "Epoch 11/30, Validation Loss: 0.6393\n",
      "Epoch 12/30, Validation Loss: 0.6339\n",
      "Epoch 13/30, Validation Loss: 0.6261\n",
      "Epoch 14/30, Validation Loss: 0.6545\n",
      "Epoch 15/30, Validation Loss: 0.6269\n",
      "Epoch 16/30, Validation Loss: 0.6218\n",
      "Epoch 17/30, Validation Loss: 0.6409\n",
      "Epoch 18/30, Validation Loss: 0.6232\n",
      "Epoch 19/30, Validation Loss: 0.6247\n",
      "Epoch 20/30, Validation Loss: 0.6274\n",
      "Epoch 21/30, Validation Loss: 0.6236\n",
      "Early stopping triggered.\n",
      "Epoch 1/70, Validation Loss: 0.5996\n",
      "Epoch 2/70, Validation Loss: 0.5816\n",
      "Epoch 3/70, Validation Loss: 0.5779\n",
      "Epoch 4/70, Validation Loss: 0.5771\n",
      "Epoch 5/70, Validation Loss: 0.5892\n",
      "Epoch 6/70, Validation Loss: 0.6145\n",
      "Epoch 7/70, Validation Loss: 0.6399\n",
      "Epoch 8/70, Validation Loss: 0.6703\n",
      "Epoch 9/70, Validation Loss: 0.6870\n",
      "Early stopping triggered.\n",
      "✅ Final fine-tuned model saved at: C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/Models/American_to_African/ResNet50_ft_final_model.pth\n",
      "✅ Test Accuracy after Fine-Tuning: 0.6155\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "FROZEN_EPOCHS = 30\n",
    "FINE_TUNE_EPOCHS = 70\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "FINE_TUNE_LR = 1e-5\n",
    "BEST_MODEL_PATH = 'C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/Models/American_to_African/ResNet50_ft_best_model.pth'\n",
    "FINAL_MODEL_PATH = 'C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/Models/American_to_African/ResNet50_ft_final_model.pth'\n",
    "\n",
    "# Transforms\n",
    "transform = Compose([\n",
    "    Resize((224, 224)),  # ResNet50 typically uses 224x224 images\n",
    "    ToTensor(),\n",
    "    Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset & Dataloaders (Updated paths)\n",
    "train_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/DatasetAmericantoAfrican/train', transform=transform)\n",
    "val_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/DatasetAmericantoAfrican/valid', transform=transform)\n",
    "test_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/DatasetAmericantoAfrican/test', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Load pretrained ResNet50 and freeze feature layers\n",
    "base_model = models.resnet101(pretrained=True)  # ResNet50\n",
    "base_model = base_model.to(device)  # Ensure the model is on the correct device\n",
    "\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False  # Freeze all layers initially\n",
    "\n",
    "# Get the correct number of features from the fc layer (Fully connected layer)\n",
    "num_ftrs = base_model.fc.in_features  # Get input features to the first Linear layer (fc layer)\n",
    "\n",
    "# Replace the classifier part of the model (fully connected layer)\n",
    "base_model.fc = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 512),  # Use num_ftrs for the input size of the linear layer\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(512, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "model = base_model.to(device)  # Ensure the model is on the correct device\n",
    "\n",
    "# Loss & optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=LEARNING_RATE)  # Update optimizer to work with the new classifier\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "\n",
    "# Early Stopping Class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, path=BEST_MODEL_PATH):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.best_model = None\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            torch.save(self.best_model, self.path)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        return self.counter >= self.patience\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "# Accuracy Function\n",
    "def compute_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = (outputs > 0.5).int().squeeze()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs):\n",
    "    early_stopping = EarlyStopping(patience=5)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        val_loss = evaluate(model, val_loader, criterion)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if early_stopping(val_loss, model):\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Train model with frozen layers\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, FROZEN_EPOCHS)\n",
    "\n",
    "# Load best model before fine-tuning\n",
    "model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "\n",
    "# Unfreeze all layers in the classifier and some feature layers\n",
    "for param in list(model.parameters())[-50:]:\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Recompile optimizer with updated params\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=FINE_TUNE_LR)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "\n",
    "# Fine-tune the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, FINE_TUNE_EPOCHS)\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), FINAL_MODEL_PATH)\n",
    "print(f\"✅ Final fine-tuned model saved at: {FINAL_MODEL_PATH}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_acc = compute_accuracy(model, test_loader)\n",
    "print(f\"✅ Test Accuracy after Fine-Tuning: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 ResNet 152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vb11574\\AppData\\Local\\miniconda3\\envs\\salmo\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet152-394f9c45.pth\" to C:\\Users\\vb11574/.cache\\torch\\hub\\checkpoints\\resnet152-394f9c45.pth\n",
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Validation Loss: 0.6597\n",
      "Epoch 2/30, Validation Loss: 0.6501\n",
      "Epoch 3/30, Validation Loss: 0.6461\n",
      "Epoch 4/30, Validation Loss: 0.6429\n",
      "Epoch 5/30, Validation Loss: 0.6411\n",
      "Epoch 6/30, Validation Loss: 0.6444\n",
      "Epoch 7/30, Validation Loss: 0.6617\n",
      "Epoch 8/30, Validation Loss: 0.6445\n",
      "Epoch 9/30, Validation Loss: 0.6466\n",
      "Epoch 10/30, Validation Loss: 0.6320\n",
      "Epoch 11/30, Validation Loss: 0.6331\n",
      "Epoch 12/30, Validation Loss: 0.6344\n",
      "Epoch 13/30, Validation Loss: 0.6315\n",
      "Epoch 14/30, Validation Loss: 0.6308\n",
      "Epoch 15/30, Validation Loss: 0.6306\n",
      "Epoch 16/30, Validation Loss: 0.6313\n",
      "Epoch 17/30, Validation Loss: 0.6300\n",
      "Epoch 18/30, Validation Loss: 0.6304\n",
      "Epoch 19/30, Validation Loss: 0.6291\n",
      "Epoch 20/30, Validation Loss: 0.6298\n",
      "Epoch 21/30, Validation Loss: 0.6305\n",
      "Epoch 22/30, Validation Loss: 0.6283\n",
      "Epoch 23/30, Validation Loss: 0.6294\n",
      "Epoch 24/30, Validation Loss: 0.6307\n",
      "Epoch 25/30, Validation Loss: 0.6316\n",
      "Epoch 26/30, Validation Loss: 0.6275\n",
      "Epoch 27/30, Validation Loss: 0.6296\n",
      "Epoch 28/30, Validation Loss: 0.6300\n",
      "Epoch 29/30, Validation Loss: 0.6279\n",
      "Epoch 30/30, Validation Loss: 0.6283\n",
      "Epoch 1/70, Validation Loss: 0.6188\n",
      "Epoch 2/70, Validation Loss: 0.6033\n",
      "Epoch 3/70, Validation Loss: 0.5974\n",
      "Epoch 4/70, Validation Loss: 0.5829\n",
      "Epoch 5/70, Validation Loss: 0.5821\n",
      "Epoch 6/70, Validation Loss: 0.6221\n",
      "Epoch 7/70, Validation Loss: 0.6463\n",
      "Epoch 8/70, Validation Loss: 0.6709\n",
      "Epoch 9/70, Validation Loss: 0.6812\n",
      "Epoch 10/70, Validation Loss: 0.7073\n",
      "Early stopping triggered.\n",
      "✅ Final fine-tuned model saved at: C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/Models/American_to_African/ResNet50_ft_final_model.pth\n",
      "✅ Test Accuracy after Fine-Tuning: 0.4710\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "FROZEN_EPOCHS = 30\n",
    "FINE_TUNE_EPOCHS = 70\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "FINE_TUNE_LR = 1e-5\n",
    "BEST_MODEL_PATH = 'C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/Models/American_to_African/ResNet50_ft_best_model.pth'\n",
    "FINAL_MODEL_PATH = 'C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/Models/American_to_African/ResNet50_ft_final_model.pth'\n",
    "\n",
    "# Transforms\n",
    "transform = Compose([\n",
    "    Resize((224, 224)),  # ResNet50 typically uses 224x224 images\n",
    "    ToTensor(),\n",
    "    Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset & Dataloaders (Updated paths)\n",
    "train_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/DatasetAmericantoAfrican/train', transform=transform)\n",
    "val_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/DatasetAmericantoAfrican/valid', transform=transform)\n",
    "test_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/TransferLearning/DatasetAmericantoAfrican/test', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Load pretrained ResNet50 and freeze feature layers\n",
    "base_model = models.resnet152(pretrained=True)  # ResNet50\n",
    "base_model = base_model.to(device)  # Ensure the model is on the correct device\n",
    "\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False  # Freeze all layers initially\n",
    "\n",
    "# Get the correct number of features from the fc layer (Fully connected layer)\n",
    "num_ftrs = base_model.fc.in_features  # Get input features to the first Linear layer (fc layer)\n",
    "\n",
    "# Replace the classifier part of the model (fully connected layer)\n",
    "base_model.fc = nn.Sequential(\n",
    "    nn.Linear(num_ftrs, 512),  # Use num_ftrs for the input size of the linear layer\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(512, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "model = base_model.to(device)  # Ensure the model is on the correct device\n",
    "\n",
    "# Loss & optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=LEARNING_RATE)  # Update optimizer to work with the new classifier\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "\n",
    "# Early Stopping Class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, path=BEST_MODEL_PATH):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.best_model = None\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            torch.save(self.best_model, self.path)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        return self.counter >= self.patience\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "# Accuracy Function\n",
    "def compute_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = (outputs > 0.5).int().squeeze()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs):\n",
    "    early_stopping = EarlyStopping(patience=5)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        val_loss = evaluate(model, val_loader, criterion)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if early_stopping(val_loss, model):\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Train model with frozen layers\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, FROZEN_EPOCHS)\n",
    "\n",
    "# Load best model before fine-tuning\n",
    "model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "\n",
    "# Unfreeze all layers in the classifier and some feature layers\n",
    "for param in list(model.parameters())[-50:]:\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Recompile optimizer with updated params\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=FINE_TUNE_LR)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "\n",
    "# Fine-tune the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, FINE_TUNE_EPOCHS)\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), FINAL_MODEL_PATH)\n",
    "print(f\"✅ Final fine-tuned model saved at: {FINAL_MODEL_PATH}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_acc = compute_accuracy(model, test_loader)\n",
    "print(f\"✅ Test Accuracy after Fine-Tuning: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 EfficientNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 EfficientNet B0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Validation Loss: 0.6591\n",
      "Epoch 2/30, Validation Loss: 0.6533\n",
      "Epoch 3/30, Validation Loss: 0.6459\n",
      "Epoch 4/30, Validation Loss: 0.6415\n",
      "Epoch 5/30, Validation Loss: 0.6394\n",
      "Epoch 6/30, Validation Loss: 0.6388\n",
      "Epoch 7/30, Validation Loss: 0.6338\n",
      "Epoch 8/30, Validation Loss: 0.6343\n",
      "Epoch 9/30, Validation Loss: 0.6273\n",
      "Epoch 10/30, Validation Loss: 0.6391\n",
      "Epoch 11/30, Validation Loss: 0.6298\n",
      "Epoch 12/30, Validation Loss: 0.6296\n",
      "Epoch 13/30, Validation Loss: 0.6198\n",
      "Epoch 14/30, Validation Loss: 0.6166\n",
      "Epoch 15/30, Validation Loss: 0.6189\n",
      "Epoch 16/30, Validation Loss: 0.6213\n",
      "Epoch 17/30, Validation Loss: 0.6166\n",
      "Epoch 18/30, Validation Loss: 0.6146\n",
      "Epoch 19/30, Validation Loss: 0.6246\n",
      "Epoch 20/30, Validation Loss: 0.6164\n",
      "Epoch 21/30, Validation Loss: 0.6249\n",
      "Epoch 22/30, Validation Loss: 0.6210\n",
      "Epoch 23/30, Validation Loss: 0.6190\n",
      "Early stopping triggered.\n",
      "Epoch 1/70, Validation Loss: 0.6102\n",
      "Epoch 2/70, Validation Loss: 0.6134\n",
      "Epoch 3/70, Validation Loss: 0.6106\n",
      "Epoch 4/70, Validation Loss: 0.6035\n",
      "Epoch 5/70, Validation Loss: 0.5992\n",
      "Epoch 6/70, Validation Loss: 0.6024\n",
      "Epoch 7/70, Validation Loss: 0.6002\n",
      "Epoch 8/70, Validation Loss: 0.6004\n",
      "Epoch 9/70, Validation Loss: 0.5961\n",
      "Epoch 10/70, Validation Loss: 0.5923\n",
      "Epoch 11/70, Validation Loss: 0.5881\n",
      "Epoch 12/70, Validation Loss: 0.5882\n",
      "Epoch 13/70, Validation Loss: 0.5949\n",
      "Epoch 14/70, Validation Loss: 0.5913\n",
      "Epoch 15/70, Validation Loss: 0.5884\n",
      "Epoch 16/70, Validation Loss: 0.5956\n",
      "Early stopping triggered.\n",
      "✅ Final fine-tuned model saved at: C:/Users/vb11574/Desktop/Salmonella_Project/Models/TransferLearning/Models/American_to_African/EfficientNetB0_ft_final_model.pth\n",
      "✅ Test Accuracy after Fine-Tuning: 0.4496\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "FROZEN_EPOCHS = 30\n",
    "FINE_TUNE_EPOCHS = 70\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "FINE_TUNE_LR = 1e-5\n",
    "BEST_MODEL_PATH = 'C:/Users/vb11574/Desktop/Salmonella_Project/Models/TransferLearning/Models/American_to_African/EfficientNetB0_ft_best_model.pth'\n",
    "FINAL_MODEL_PATH = 'C:/Users/vb11574/Desktop/Salmonella_Project/Models/TransferLearning/Models/American_to_African/EfficientNetB0_ft_final_model.pth'\n",
    "\n",
    "# Transforms\n",
    "transform = Compose([\n",
    "    Resize((224, 224)),  # MobileNetV1 typically uses 224x224 images\n",
    "    ToTensor(),\n",
    "    Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset & Dataloaders (Updated paths)\n",
    "train_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/Models/TransferLearning/DatasetAmericantoAfrican/train', transform=transform)\n",
    "val_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/Models/TransferLearning/DatasetAmericantoAfrican/valid', transform=transform)\n",
    "test_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/Models/TransferLearning/DatasetAmericantoAfrican/test', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Load pretrained MobileNetV1 and freeze feature layers\n",
    "base_model = models.efficientnet_b0(pretrained=True)  # MobileNetV1 equivalent is available as MobileNetV2 in torchvision\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False  # Freeze all layers initially\n",
    "\n",
    "# Replace the classifier part of the model (keep global average pooling)\n",
    "base_model.classifier = nn.Sequential(\n",
    "    nn.Linear(base_model.classifier[1].in_features, 512),  # base_model.classifier[1].in_features gives the correct input size\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(512, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "model = base_model.to(device)\n",
    "\n",
    "# Loss & optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "\n",
    "# Early Stopping Class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, path=BEST_MODEL_PATH):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.best_model = None\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            torch.save(self.best_model, self.path)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        return self.counter >= self.patience\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "# Accuracy Function\n",
    "def compute_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = (outputs > 0.5).int().squeeze()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs):\n",
    "    early_stopping = EarlyStopping(patience=5)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        val_loss = evaluate(model, val_loader, criterion)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if early_stopping(val_loss, model):\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Train model with frozen layers\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, FROZEN_EPOCHS)\n",
    "\n",
    "# Load best model before fine-tuning\n",
    "model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "\n",
    "# Unfreeze all layers in the classifier and some feature layers\n",
    "for param in list(model.features.parameters())[-50:]:\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Recompile optimizer with updated params\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=FINE_TUNE_LR)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "\n",
    "# Fine-tune the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, FINE_TUNE_EPOCHS)\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), FINAL_MODEL_PATH)\n",
    "print(f\"✅ Final fine-tuned model saved at: {FINAL_MODEL_PATH}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_acc = compute_accuracy(model, test_loader)\n",
    "print(f\"✅ Test Accuracy after Fine-Tuning: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 EfficientNet B3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vb11574\\AppData\\Local\\miniconda3\\envs\\salmo\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B3_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b3_rwightman-b3899882.pth\" to C:\\Users\\vb11574/.cache\\torch\\hub\\checkpoints\\efficientnet_b3_rwightman-b3899882.pth\n",
      "100%|██████████| 47.2M/47.2M [00:00<00:00, 112MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Validation Loss: 0.6672\n",
      "Epoch 2/30, Validation Loss: 0.6621\n",
      "Epoch 3/30, Validation Loss: 0.6553\n",
      "Epoch 4/30, Validation Loss: 0.6517\n",
      "Epoch 5/30, Validation Loss: 0.6481\n",
      "Epoch 6/30, Validation Loss: 0.6453\n",
      "Epoch 7/30, Validation Loss: 0.6429\n",
      "Epoch 8/30, Validation Loss: 0.6432\n",
      "Epoch 9/30, Validation Loss: 0.6373\n",
      "Epoch 10/30, Validation Loss: 0.6362\n",
      "Epoch 11/30, Validation Loss: 0.6335\n",
      "Epoch 12/30, Validation Loss: 0.6326\n",
      "Epoch 13/30, Validation Loss: 0.6322\n",
      "Epoch 14/30, Validation Loss: 0.6340\n",
      "Epoch 15/30, Validation Loss: 0.6265\n",
      "Epoch 16/30, Validation Loss: 0.6251\n",
      "Epoch 17/30, Validation Loss: 0.6350\n",
      "Epoch 18/30, Validation Loss: 0.6384\n",
      "Epoch 19/30, Validation Loss: 0.6314\n",
      "Epoch 20/30, Validation Loss: 0.6325\n",
      "Epoch 21/30, Validation Loss: 0.6342\n",
      "Early stopping triggered.\n",
      "Epoch 1/70, Validation Loss: 0.6273\n",
      "Epoch 2/70, Validation Loss: 0.6264\n",
      "Epoch 3/70, Validation Loss: 0.6266\n",
      "Epoch 4/70, Validation Loss: 0.6230\n",
      "Epoch 5/70, Validation Loss: 0.6219\n",
      "Epoch 6/70, Validation Loss: 0.6162\n",
      "Epoch 7/70, Validation Loss: 0.6177\n",
      "Epoch 8/70, Validation Loss: 0.6204\n",
      "Epoch 9/70, Validation Loss: 0.6141\n",
      "Epoch 10/70, Validation Loss: 0.6160\n",
      "Epoch 11/70, Validation Loss: 0.6109\n",
      "Epoch 12/70, Validation Loss: 0.6154\n",
      "Epoch 13/70, Validation Loss: 0.6097\n",
      "Epoch 14/70, Validation Loss: 0.6116\n",
      "Epoch 15/70, Validation Loss: 0.6078\n",
      "Epoch 16/70, Validation Loss: 0.6114\n",
      "Epoch 17/70, Validation Loss: 0.6056\n",
      "Epoch 18/70, Validation Loss: 0.6050\n",
      "Epoch 19/70, Validation Loss: 0.6073\n",
      "Epoch 20/70, Validation Loss: 0.6074\n",
      "Epoch 21/70, Validation Loss: 0.6080\n",
      "Epoch 22/70, Validation Loss: 0.6091\n",
      "Epoch 23/70, Validation Loss: 0.6063\n",
      "Early stopping triggered.\n",
      "✅ Final fine-tuned model saved at: C:/Users/vb11574/Desktop/Salmonella_Project/Models/TransferLearning/Models/American_to_African/EfficientNetB3_ft_final_model.pth\n",
      "✅ Test Accuracy after Fine-Tuning: 0.4547\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "FROZEN_EPOCHS = 30\n",
    "FINE_TUNE_EPOCHS = 70\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "FINE_TUNE_LR = 1e-5\n",
    "BEST_MODEL_PATH = 'C:/Users/vb11574/Desktop/Salmonella_Project/Models/TransferLearning/Models/American_to_African/EfficientNetB3_ft_best_model.pth'\n",
    "FINAL_MODEL_PATH = 'C:/Users/vb11574/Desktop/Salmonella_Project/Models/TransferLearning/Models/American_to_African/EfficientNetB3_ft_final_model.pth'\n",
    "\n",
    "# Transforms\n",
    "transform = Compose([\n",
    "    Resize((224, 224)),  # MobileNetV1 typically uses 224x224 images\n",
    "    ToTensor(),\n",
    "    Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset & Dataloaders (Updated paths)\n",
    "train_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/Models/TransferLearning/DatasetAmericantoAfrican/train', transform=transform)\n",
    "val_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/Models/TransferLearning/DatasetAmericantoAfrican/valid', transform=transform)\n",
    "test_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/Models/TransferLearning/DatasetAmericantoAfrican/test', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Load pretrained MobileNetV1 and freeze feature layers\n",
    "base_model = models.efficientnet_b3(pretrained=True)  # MobileNetV1 equivalent is available as MobileNetV2 in torchvision\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False  # Freeze all layers initially\n",
    "\n",
    "# Replace the classifier part of the model (keep global average pooling)\n",
    "base_model.classifier = nn.Sequential(\n",
    "    nn.Linear(base_model.classifier[1].in_features, 512),  # base_model.classifier[1].in_features gives the correct input size\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(512, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "model = base_model.to(device)\n",
    "\n",
    "# Loss & optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "\n",
    "# Early Stopping Class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, path=BEST_MODEL_PATH):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.best_model = None\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            torch.save(self.best_model, self.path)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        return self.counter >= self.patience\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "# Accuracy Function\n",
    "def compute_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = (outputs > 0.5).int().squeeze()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs):\n",
    "    early_stopping = EarlyStopping(patience=5)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        val_loss = evaluate(model, val_loader, criterion)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if early_stopping(val_loss, model):\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Train model with frozen layers\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, FROZEN_EPOCHS)\n",
    "\n",
    "# Load best model before fine-tuning\n",
    "model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "\n",
    "# Unfreeze all layers in the classifier and some feature layers\n",
    "for param in list(model.features.parameters())[-50:]:\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Recompile optimizer with updated params\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=FINE_TUNE_LR)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "\n",
    "# Fine-tune the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, FINE_TUNE_EPOCHS)\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), FINAL_MODEL_PATH)\n",
    "print(f\"✅ Final fine-tuned model saved at: {FINAL_MODEL_PATH}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_acc = compute_accuracy(model, test_loader)\n",
    "print(f\"✅ Test Accuracy after Fine-Tuning: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 EfficientNet B7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vb11574\\AppData\\Local\\miniconda3\\envs\\salmo\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B7_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B7_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b7_lukemelas-c5b4e57e.pth\" to C:\\Users\\vb11574/.cache\\torch\\hub\\checkpoints\\efficientnet_b7_lukemelas-c5b4e57e.pth\n",
      "100%|██████████| 255M/255M [00:02<00:00, 117MB/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Validation Loss: 0.6837\n",
      "Epoch 2/30, Validation Loss: 0.6662\n",
      "Epoch 3/30, Validation Loss: 0.6580\n",
      "Epoch 4/30, Validation Loss: 0.6538\n",
      "Epoch 5/30, Validation Loss: 0.6498\n",
      "Epoch 6/30, Validation Loss: 0.6474\n",
      "Epoch 7/30, Validation Loss: 0.6470\n",
      "Epoch 8/30, Validation Loss: 0.6461\n",
      "Epoch 9/30, Validation Loss: 0.6475\n",
      "Epoch 10/30, Validation Loss: 0.6472\n",
      "Epoch 11/30, Validation Loss: 0.6463\n",
      "Epoch 12/30, Validation Loss: 0.6432\n",
      "Epoch 13/30, Validation Loss: 0.6442\n",
      "Epoch 14/30, Validation Loss: 0.6473\n",
      "Epoch 15/30, Validation Loss: 0.6448\n",
      "Epoch 16/30, Validation Loss: 0.6428\n",
      "Epoch 17/30, Validation Loss: 0.6405\n",
      "Epoch 18/30, Validation Loss: 0.6419\n",
      "Epoch 19/30, Validation Loss: 0.6416\n",
      "Epoch 20/30, Validation Loss: 0.6435\n",
      "Epoch 21/30, Validation Loss: 0.6422\n",
      "Epoch 22/30, Validation Loss: 0.6418\n",
      "Early stopping triggered.\n",
      "Epoch 1/70, Validation Loss: 0.6349\n",
      "Epoch 2/70, Validation Loss: 0.6291\n",
      "Epoch 3/70, Validation Loss: 0.6267\n",
      "Epoch 4/70, Validation Loss: 0.6265\n",
      "Epoch 5/70, Validation Loss: 0.6234\n",
      "Epoch 6/70, Validation Loss: 0.6183\n",
      "Epoch 7/70, Validation Loss: 0.6163\n",
      "Epoch 8/70, Validation Loss: 0.6175\n",
      "Epoch 9/70, Validation Loss: 0.6189\n",
      "Epoch 10/70, Validation Loss: 0.6220\n",
      "Epoch 11/70, Validation Loss: 0.6213\n",
      "Epoch 12/70, Validation Loss: 0.6217\n",
      "Early stopping triggered.\n",
      "✅ Final fine-tuned model saved at: C:/Users/vb11574/Desktop/Salmonella_Project/Models/TransferLearning/Models/American_to_African/EfficientNetB7_ft_final_model.pth\n",
      "✅ Test Accuracy after Fine-Tuning: 0.4510\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "import copy\n",
    "import os\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "FROZEN_EPOCHS = 30\n",
    "FINE_TUNE_EPOCHS = 70\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-4\n",
    "FINE_TUNE_LR = 1e-5\n",
    "BEST_MODEL_PATH = 'C:/Users/vb11574/Desktop/Salmonella_Project/Models/TransferLearning/Models/American_to_African/EfficientNetB7_ft_best_model.pth'\n",
    "FINAL_MODEL_PATH = 'C:/Users/vb11574/Desktop/Salmonella_Project/Models/TransferLearning/Models/American_to_African/EfficientNetB7_ft_final_model.pth'\n",
    "\n",
    "# Transforms\n",
    "transform = Compose([\n",
    "    Resize((224, 224)),  # MobileNetV1 typically uses 224x224 images\n",
    "    ToTensor(),\n",
    "    Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset & Dataloaders (Updated paths)\n",
    "train_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/Models/TransferLearning/DatasetAmericantoAfrican/train', transform=transform)\n",
    "val_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/Models/TransferLearning/DatasetAmericantoAfrican/valid', transform=transform)\n",
    "test_dataset = ImageFolder('C:/Users/vb11574/Desktop/Salmonella_Project/Models/TransferLearning/DatasetAmericantoAfrican/test', transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Load pretrained MobileNetV1 and freeze feature layers\n",
    "base_model = models.efficientnet_b7(pretrained=True)  # MobileNetV1 equivalent is available as MobileNetV2 in torchvision\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False  # Freeze all layers initially\n",
    "\n",
    "# Replace the classifier part of the model (keep global average pooling)\n",
    "base_model.classifier = nn.Sequential(\n",
    "    nn.Linear(base_model.classifier[1].in_features, 512),  # base_model.classifier[1].in_features gives the correct input size\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(512, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "model = base_model.to(device)\n",
    "\n",
    "# Loss & optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "\n",
    "# Early Stopping Class\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, path=BEST_MODEL_PATH):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.best_model = None\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss:\n",
    "            self.best_loss = val_loss\n",
    "            self.best_model = copy.deepcopy(model.state_dict())\n",
    "            torch.save(self.best_model, self.path)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        return self.counter >= self.patience\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "# Accuracy Function\n",
    "def compute_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            preds = (outputs > 0.5).int().squeeze()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return correct / total\n",
    "\n",
    "# Training Function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs):\n",
    "    early_stopping = EarlyStopping(patience=5)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        val_loss = evaluate(model, val_loader, criterion)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        if early_stopping(val_loss, model):\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Train model with frozen layers\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, FROZEN_EPOCHS)\n",
    "\n",
    "# Load best model before fine-tuning\n",
    "model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "\n",
    "# Unfreeze all layers in the classifier and some feature layers\n",
    "for param in list(model.features.parameters())[-50:]:\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Recompile optimizer with updated params\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=FINE_TUNE_LR)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "\n",
    "# Fine-tune the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, FINE_TUNE_EPOCHS)\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), FINAL_MODEL_PATH)\n",
    "print(f\"✅ Final fine-tuned model saved at: {FINAL_MODEL_PATH}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_acc = compute_accuracy(model, test_loader)\n",
    "print(f\"✅ Test Accuracy after Fine-Tuning: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salmo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
